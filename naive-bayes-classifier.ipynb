{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Naive Bayes [20 marks]\n",
    "\n",
    "Student Name:Han Wang\n",
    "\n",
    "Student ID:1041260\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 8 April 2022 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count)\n",
    "<ul>\n",
    "    <li>one day late, -2.0;</li>\n",
    "    <li>two days late, -4.0;</li>\n",
    "    <li>three days late, -6.0;</li>\n",
    "    <li>four days late, -8.0;</li>\n",
    "    <li>five days late, -10.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>: 20% of mark for class. \n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/124196/pages/python-and-jupyter-notebooks?module_item_id=3512182) on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. You can use any Python built-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should implement functions for the skeletons listed below. You may implement any number of additional (helper) functions. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 4 marks for unreadable or exessively inefficient code.\n",
    "\n",
    "8 of the marks available for this Project will be assigned to whether the five specified Python functions work in a manner consistent with the materials from COMP90049. Any other implementation will not be directly assessed (except insofar as it is required to make these five functions work correctly).\n",
    "\n",
    "12 of the marks will be assigned to your responses to the questions, in terms of both accuracy and insightfulness. We will be looking for evidence that you have an implementation that allows you to explore the problem, but also that you have thought deeply about the data and the behaviour of the Naive Bayes classifier.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (Piazza -> Assignments -> A2); we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/124196/modules#module_662096\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>missing Authorship Declaration at the bottom of the page, -10.0\n",
    "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -5.0\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Base code [8 marks]\n",
    "\n",
    "Instructions\n",
    "1. Do **not** shuffle the data set\n",
    "2. Treat the features as continuous numeric and use them as provided (e.g., do **not** convert them to other feature types, such as discrete ones). Implement a Naive Bayes classifier with appropriate likelihood function for the data\n",
    "3. You should implement the Naive Bayes classifier from scratch. Do **not** use existing implementations/learning algorithms.\n",
    "4. Apart from the instructions in point 3, you may use libraries to help you with data reading, representation, maths or evaluation\n",
    "5. Ensure that all and only required information is printed, as indicated in the final three code cells. Failure to adhere to print the required information will result in **[-1 mark]** per case. *(We don't mind details like you print a list or several numbers -- just make sure the information is displayed so that it's easily accessible)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a csv file and read the data into a useable format [1 mark]\n",
    "def preprocess(filename):\n",
    "    data = pd.read_csv(filename).iloc[:,1:]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model [3 marks]\n",
    "def train(train_feature, train_label, label_list):\n",
    "    prior = {}\n",
    "    size = train_label.size\n",
    "    for i in label_list:\n",
    "        prior[i] = np.log2(train_label.value_counts()[i]/size)\n",
    "    likelihood = {}\n",
    "    for i in label_list:\n",
    "        likelihood[i] = []\n",
    "        columns = train_feature.columns\n",
    "        for j in columns:\n",
    "            seperate_by_class = train_feature[j][train_label == i]\n",
    "            likelihood[i].append({\"mean\":seperate_by_class.mean(), \"std\":seperate_by_class.std()})\n",
    "    model = (prior, likelihood)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function help to calculate the prob of normal distribution\n",
    "def Gaussian(value, mean, stand):\n",
    "    prediction = (1 / (math.sqrt(math.pi * 2) * stand)) * math.exp(-((value - mean) ** 2 / (2 * stand ** 2)))\n",
    "    if prediction == 0:\n",
    "        prediction += 10**(-9)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for a set of instances, based on a trained model [2 marks]\n",
    "def predict(test_feature, model, label_list):\n",
    "    prediction = []\n",
    "    probility = []\n",
    "    feature = test_feature.shape[0]\n",
    "    for i in range(feature):\n",
    "        posterior = {}\n",
    "        for j in label_list:\n",
    "            prob = model[0][j]\n",
    "            for k in range (test_feature.iloc[i,:].size):\n",
    "                value = test_feature.iloc[i,:][k]\n",
    "                prob += np.log2(Gaussian(value, model[1][j][k]['mean'], model[1][j][k]['std']))\n",
    "            posterior[j] = prob\n",
    "        keywords = posterior.get\n",
    "        prediction.append(max(posterior, key = keywords))\n",
    "        probility.append(posterior)\n",
    "    return prediction, probility\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(matrix, class1):\n",
    "    index1 = [\"TRUE\", \"FALSE\"] \n",
    "    column1 = [\"POSITIVE\", \"NEGATIVE\", \"PRECISION\", \"RECALL\", \"F1-SCORE\"]\n",
    "    output = pd.DataFrame(index = index1, columns = column1)\n",
    "    tp = matrix.loc[class1, class1]\n",
    "    fp = matrix.loc[:,class1].sum() - tp\n",
    "    fn = matrix.loc[class1,:].sum() - tp\n",
    "    tn = matrix.sum().sum() - tp - fp - fn\n",
    "    output.loc[\"TRUE\", \"POSITIVE\"] = tp\n",
    "    output.loc[\"TRUE\", \"NEGATIVE\"] = tn\n",
    "    output.loc[\"FALSE\", \"POSITIVE\"] = fp\n",
    "    output.loc[\"FALSE\", \"NEGATIVE\"] = fn\n",
    "    if tp == 0:\n",
    "        output.loc[\"TRUE\", \"PRECISION\"] = 0\n",
    "        output.loc[\"TRUE\", \"RECALL\"] = 0\n",
    "        output.loc[\"TRUE\", \"F1-SCORE\"] = 0\n",
    "    else:\n",
    "        output.loc[\"TRUE\", \"PRECISION\"] = tp /(tp + fp)\n",
    "        output.loc[\"TRUE\", \"RECALL\"] = tp /(tp + fn)\n",
    "        output.loc[\"TRUE\", \"F1-SCORE\"] = 2 * (tp /(tp + fp)) * (tp /(tp + fn)) / ((tp /(tp + fp)) + (tp /(tp + fn)))\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions [1 mark]\n",
    "def evaluate(predicted, true, label_list, model):\n",
    "    result = pd.DataFrame()\n",
    "    result[\"Predicted_Label\"] = predicted\n",
    "    true.reset_index(inplace = True, drop = True)\n",
    "    result[\"True_Label\"] = true\n",
    "    true_length = (result[\"True_Label\"] == result[\"Predicted_Label\"]).value_counts()[True]\n",
    "    labels = result.iloc[:,1].value_counts().keys()\n",
    "    matrix = pd.DataFrame(index = labels, columns = labels)\n",
    "    for i in labels:\n",
    "        for j in labels:\n",
    "            matrix.loc[i,j] = result[(result[\"True_Label\"] == i) & (result[\"Predicted_Label\"] == j)].shape[0]\n",
    "    data_dict = {}\n",
    "    precison = 0\n",
    "    recall = 0\n",
    "    for i in label_list:\n",
    "        data_dict[i] = calculate(matrix, i)\n",
    "        precison += data_dict[i].loc[\"TRUE\", \"PRECISION\"]\n",
    "        recall += data_dict[i].loc[\"TRUE\", \"RECALL\"]\n",
    "    length = len(label_list)\n",
    "    print(\"Acurracy: \", true_length / result.shape[0])\n",
    "    print(\"Confusion:\")\n",
    "    print(matrix)\n",
    "    print(\"Precision: \", precison / length)\n",
    "    print(\"Recall: \", recall / length)\n",
    "    print(\"Zero-R accurate: \", 2**max(model[0].values()))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.792\n",
      "Confusion:\n",
      "           objective subjective\n",
      "objective        555         80\n",
      "subjective       128        237\n",
      "Precision:  0.7801127887266698\n",
      "Recall:  0.7616654082623233\n",
      "Zero-R accurate:  0.635\n",
      "Feature vectors of instances [0, 1, 2]:  \n",
      " totalWordsCount     109\n",
      "FW                    8\n",
      "JJ                    0\n",
      "JJR                   0\n",
      "LS                    0\n",
      "NN                    0\n",
      "NNPS                 12\n",
      "NNS                   0\n",
      "POS                   1\n",
      "PRP                   2\n",
      "PRP$                  2\n",
      "RBR                   0\n",
      "RBS                   2\n",
      "UH                    0\n",
      "VBD                   0\n",
      "VBZ                   0\n",
      "Quotes                0\n",
      "questionmarks         0\n",
      "exclamationmarks      0\n",
      "fullstops             4\n",
      "pronouns1st           0\n",
      "pronouns2nd           0\n",
      "pronouns3rd           3\n",
      "Name: 0, dtype: object \n",
      " totalWordsCount     309\n",
      "FW                   35\n",
      "JJ                    0\n",
      "JJR                   0\n",
      "LS                    2\n",
      "NN                    1\n",
      "NNPS                 17\n",
      "NNS                   0\n",
      "POS                   7\n",
      "PRP                   5\n",
      "PRP$                 16\n",
      "RBR                   0\n",
      "RBS                   0\n",
      "UH                    0\n",
      "VBD                   7\n",
      "VBZ                   0\n",
      "Quotes                7\n",
      "questionmarks         0\n",
      "exclamationmarks      0\n",
      "fullstops            19\n",
      "pronouns1st           1\n",
      "pronouns2nd           0\n",
      "pronouns3rd          10\n",
      "Name: 1, dtype: object \n",
      " totalWordsCount     149\n",
      "FW                   15\n",
      "JJ                    0\n",
      "JJR                   0\n",
      "LS                    0\n",
      "NN                    1\n",
      "NNPS                  4\n",
      "NNS                   0\n",
      "POS                   4\n",
      "PRP                   0\n",
      "PRP$                  7\n",
      "RBR                   0\n",
      "RBS                   1\n",
      "UH                    0\n",
      "VBD                   2\n",
      "VBZ                   0\n",
      "Quotes                0\n",
      "questionmarks         0\n",
      "exclamationmarks      0\n",
      "fullstops             6\n",
      "pronouns1st           0\n",
      "pronouns2nd           0\n",
      "pronouns3rd           2\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Number of instances (N):  1000\n",
      "Number of features (F):  23\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'objective': -216.9730375656972, 'subjective': -119.4870733377582}\n",
      "Predicted class ID for instance N-3:  subjective\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'objective': -95.74733909477813, 'subjective': -120.20686543125724}\n",
      "Predicted class ID for instance N-2:  objective\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'objective': -108.5113138629028, 'subjective': -108.08266335875754}\n",
      "Predicted class ID for instance N-1:  subjective\n"
     ]
    }
   ],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full OBJECTIVITY data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply your NB model to the OBJECTIVITY data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "data = preprocess(\"objectivity.csv\")\n",
    "label_list = list(data.Label.unique())\n",
    "model = train(data.drop(columns = \"Label\"), data[\"Label\"], label_list)\n",
    "prediction, prob_prediction = predict(data.drop(columns = \"Label\"), model, label_list)\n",
    "\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "evaluate(prediction, data.Label, label_list, model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", \"\\n\", data.iloc[0,1:], \"\\n\", data.iloc[1,1:], \"\\n\", data.iloc[2,1:])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", data.shape[0])\n",
    "print(\"Number of features (F): \", data.shape[1] - 1)\n",
    "print(\"Number of labels (L): \", len(label_list))\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", prob_prediction[data.shape[0] - 3])\n",
    "print(\"Predicted class ID for instance N-3: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 3][x]))\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", prob_prediction[data.shape[0] - 2])\n",
    "print(\"Predicted class ID for instance N-2: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 2][x]))\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", prob_prediction[data.shape[0] - 1])\n",
    "print(\"Predicted class ID for instance N-1: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 1][x]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.8016\n",
      "Confusion:\n",
      "      <=50K >50K\n",
      "<=50K  1817   96\n",
      ">50K    400  187\n",
      "Precision:  0.7401766943837453\n",
      "Recall:  0.6341930180928304\n",
      "Zero-R accurate:  0.7652\n",
      "Feature vectors of instances [0, 1, 2]:  \n",
      " Age                   31\n",
      "fnlwgt            142470\n",
      "Education-num         13\n",
      "Capital-gain           0\n",
      "Capital-loss           0\n",
      "Hours-per-week        40\n",
      "Name: 0, dtype: object \n",
      " Age                   31\n",
      "fnlwgt            323069\n",
      "Education-num          9\n",
      "Capital-gain           0\n",
      "Capital-loss           0\n",
      "Hours-per-week        20\n",
      "Name: 1, dtype: object \n",
      " Age                   25\n",
      "fnlwgt            122489\n",
      "Education-num         13\n",
      "Capital-gain           0\n",
      "Capital-loss        1726\n",
      "Hours-per-week        60\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Number of instances (N):  2500\n",
      "Number of features (F):  6\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'<=50K': -56.18429822582242, '>50K': -59.80824868082521}\n",
      "Predicted class ID for instance N-3:  <=50K\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'<=50K': -53.011261264741684, '>50K': -60.39969804121705}\n",
      "Predicted class ID for instance N-2:  <=50K\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'<=50K': -52.52464711542919, '>50K': -58.83296788844591}\n",
      "Predicted class ID for instance N-1:  <=50K\n"
     ]
    }
   ],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full ADULT data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply your NB model to the ADULT data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "data = preprocess(\"adult.csv\")\n",
    "label_list = list(data.Label.unique())\n",
    "model = train(data.drop(columns = \"Label\"), data[\"Label\"], label_list)\n",
    "prediction, prob_prediction = predict(data.drop(columns = \"Label\"), model, label_list)\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "\n",
    "evaluate(prediction, data.Label, label_list, model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", \"\\n\", data.iloc[0,1:], \"\\n\", data.iloc[1,1:], \"\\n\", data.iloc[2,1:])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", data.shape[0])\n",
    "print(\"Number of features (F): \", data.shape[1] - 1)\n",
    "print(\"Number of labels (L): \", len(label_list))\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", prob_prediction[data.shape[0] - 3])\n",
    "print(\"Predicted class ID for instance N-3: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 3][x]))\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", prob_prediction[data.shape[0] - 2])\n",
    "print(\"Predicted class ID for instance N-2: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 2][x]))\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", prob_prediction[data.shape[0] - 1])\n",
    "print(\"Predicted class ID for instance N-1: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 1][x]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.35222052067381315\n",
      "Confusion:\n",
      "      8   2   3   1   0 >24 24\n",
      "8    96  33  44   6  17   5  7\n",
      "2    31  59  46   9   2   7  3\n",
      "3    29  28  42   9   3   1  0\n",
      "1    24  17  24  13   4   0  6\n",
      "0    14   5   4   7  11   0  3\n",
      ">24  11   2   4   2   2   6  1\n",
      "24    3   4   1   3   1   1  3\n",
      "Precision:  0.2979247813986058\n",
      "Recall:  0.28740680384219963\n",
      "Zero-R accurate:  0.31852986217457885\n",
      "Feature vectors of instances [0, 1, 2]:  \n",
      " Service time                   18\n",
      "Age                            50\n",
      "Work load Average/day     239.554\n",
      "Hit target                     97\n",
      "Son                             1\n",
      "Pet                             0\n",
      "Weight                         98\n",
      "Height                        178\n",
      "Body mass index                31\n",
      "Name: 0, dtype: object \n",
      " Service time                   18\n",
      "Age                            38\n",
      "Work load Average/day     239.554\n",
      "Hit target                     97\n",
      "Son                             0\n",
      "Pet                             0\n",
      "Weight                         89\n",
      "Height                        170\n",
      "Body mass index                31\n",
      "Name: 1, dtype: object \n",
      " Service time                   13\n",
      "Age                            33\n",
      "Work load Average/day     239.554\n",
      "Hit target                     97\n",
      "Son                             2\n",
      "Pet                             1\n",
      "Weight                         90\n",
      "Height                        172\n",
      "Body mass index                30\n",
      "Name: 2, dtype: object\n",
      "\n",
      "Number of instances (N):  653\n",
      "Number of features (F):  9\n",
      "Number of labels (L):  7\n",
      "\n",
      "\n",
      "Predicted class probabilities for instance N-3:  {'0': -50.411816550060514, '2': -65.38830902275444, '8': -52.85001073919146, '>24': -96.46360580338053, '1': -71.45286703811912, '3': -62.20736663925894, '24': -312.13207673773894}\n",
      "Predicted class ID for instance N-3:  0\n",
      "\n",
      "Predicted class probabilities for instance N-2:  {'0': -40.62666884674171, '2': -42.29844622254875, '8': -40.27161042419796, '>24': -48.493724438226145, '1': -42.73758675351361, '3': -41.24489824836414, '24': -57.26413908670366}\n",
      "Predicted class ID for instance N-2:  8\n",
      "\n",
      "Predicted class probabilities for instance N-1:  {'0': -41.19391787325953, '2': -39.75909302942923, '8': -40.49918280147374, '>24': -40.459591807328785, '1': -38.83546785269001, '3': -40.5844623194541, '24': -45.3235370885157}\n",
      "Predicted class ID for instance N-1:  1\n"
     ]
    }
   ],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full ABSENTEEISM data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply your NB model to the ABSENTEEISM data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "data = preprocess(\"absenteeism.csv\")\n",
    "label_list = list(data.Label.unique())\n",
    "model = train(data.drop(columns = \"Label\"), data[\"Label\"], label_list)\n",
    "prediction, prob_prediction = predict(data.drop(columns = \"Label\"), model, label_list)\n",
    "\n",
    "\n",
    "\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "\n",
    "\n",
    "\n",
    "evaluate(prediction, data.Label, label_list, model)\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", \"\\n\", data.iloc[0,1:], \"\\n\", data.iloc[1,1:], \"\\n\", data.iloc[2,1:])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", data.shape[0])\n",
    "print(\"Number of features (F): \", data.shape[1] - 1)\n",
    "print(\"Number of labels (L): \", len(label_list))\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", prob_prediction[data.shape[0] - 3])\n",
    "print(\"Predicted class ID for instance N-3: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 3][x]))\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", prob_prediction[data.shape[0] - 2])\n",
    "print(\"Predicted class ID for instance N-2: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 2][x]))\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", prob_prediction[data.shape[0] - 1])\n",
    "print(\"Predicted class ID for instance N-1: \", max(prob_prediction[data.shape[0] - 2],key=lambda x:prob_prediction[data.shape[0] - 1][x]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Conceptual questions [12 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Evaluation strategy [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.79\n",
      "Confusion:\n",
      "           objective subjective\n",
      "objective        107         17\n",
      "subjective        25         51\n",
      "Precision:  0.7803030303030303\n",
      "Recall:  0.7669779286926994\n",
      "Zero-R accurate:  0.63875\n"
     ]
    }
   ],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "data = preprocess(\"objectivity.csv\")\n",
    "label_list = list(data.Label.unique())\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.drop(columns = \"Label\"), data.Label, test_size=0.2, random_state=42)\n",
    "model = train(x_train, y_train, label_list)\n",
    "prediction, _ = predict(x_test, model, label_list)\n",
    "evaluate(prediction, y_test, label_list, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer of 100-150 words in this cell.\n",
    "\n",
    "By compare the three results, all of them decrease for a little bit. Actually, the accurate rate of test will never higher than the accurate rate of train, because the model is optimized by the train data. This means that the test uses the model of train, so it cannot as accurate as the model of itself. If there are some new instances, which means never happen in train models, the result will not accurate. As a consequence, using train data to test is a wrong choice because the result will over optimistic. In the real world, the test data will not the same as train data, it is better to choose new data so that we can learn from the test.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Feature standardization [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer of 100-150 words in this cell.\n",
    "\n",
    "Feature standardization means standardize data by subtracting the mean and then dividing by the variance. To make the value have zero-mean and unit-variance. Firstly, we need to calculate the distribution mean and standard deviation. And then, divide the values of every instance by its standard deviation.\n",
    "For Gaussian Naïve Bayes classifier, the probability of the value uses the function(Probability Density Function), which called PDF. The only difference between PDF and Normal PDF is that add the standard normal distribution, which is scaled by mean. But the value mean + 0.5 standard deviation is the same as the origin probability. Consequently, in Gaussian Naïve Bayes classifier, the probability will not change, and the result will not change. So, we can say the Gaussian Naïve Bayes classifier have standardization by itself. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Naive Bayes assumptions [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "a = data[\"FW\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalWordsCount</th>\n",
       "      <th>FW</th>\n",
       "      <th>JJ</th>\n",
       "      <th>JJR</th>\n",
       "      <th>LS</th>\n",
       "      <th>NN</th>\n",
       "      <th>NNPS</th>\n",
       "      <th>NNS</th>\n",
       "      <th>POS</th>\n",
       "      <th>PRP</th>\n",
       "      <th>...</th>\n",
       "      <th>UH</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>Quotes</th>\n",
       "      <th>questionmarks</th>\n",
       "      <th>exclamationmarks</th>\n",
       "      <th>fullstops</th>\n",
       "      <th>pronouns1st</th>\n",
       "      <th>pronouns2nd</th>\n",
       "      <th>pronouns3rd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>totalWordsCount</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980416</td>\n",
       "      <td>0.696009</td>\n",
       "      <td>0.702140</td>\n",
       "      <td>0.791851</td>\n",
       "      <td>0.456650</td>\n",
       "      <td>0.919123</td>\n",
       "      <td>0.423759</td>\n",
       "      <td>0.793812</td>\n",
       "      <td>0.840559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790048</td>\n",
       "      <td>0.890128</td>\n",
       "      <td>0.704837</td>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.576761</td>\n",
       "      <td>0.203342</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.501285</td>\n",
       "      <td>0.564170</td>\n",
       "      <td>0.802912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FW</th>\n",
       "      <td>0.980416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.691682</td>\n",
       "      <td>0.684645</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.456005</td>\n",
       "      <td>0.914988</td>\n",
       "      <td>0.390069</td>\n",
       "      <td>0.743604</td>\n",
       "      <td>0.823196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737380</td>\n",
       "      <td>0.876908</td>\n",
       "      <td>0.711894</td>\n",
       "      <td>0.064259</td>\n",
       "      <td>0.533813</td>\n",
       "      <td>0.184856</td>\n",
       "      <td>0.878764</td>\n",
       "      <td>0.457993</td>\n",
       "      <td>0.522194</td>\n",
       "      <td>0.774438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>0.696009</td>\n",
       "      <td>0.691682</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.518882</td>\n",
       "      <td>0.645191</td>\n",
       "      <td>0.226665</td>\n",
       "      <td>0.645691</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.545510</td>\n",
       "      <td>0.578256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587884</td>\n",
       "      <td>0.615098</td>\n",
       "      <td>0.546748</td>\n",
       "      <td>-0.007403</td>\n",
       "      <td>0.483970</td>\n",
       "      <td>0.184978</td>\n",
       "      <td>0.614681</td>\n",
       "      <td>0.299160</td>\n",
       "      <td>0.426286</td>\n",
       "      <td>0.558371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJR</th>\n",
       "      <td>0.702140</td>\n",
       "      <td>0.684645</td>\n",
       "      <td>0.518882</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575935</td>\n",
       "      <td>0.332808</td>\n",
       "      <td>0.665866</td>\n",
       "      <td>0.338214</td>\n",
       "      <td>0.532190</td>\n",
       "      <td>0.601060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574393</td>\n",
       "      <td>0.592037</td>\n",
       "      <td>0.497882</td>\n",
       "      <td>-0.008925</td>\n",
       "      <td>0.420193</td>\n",
       "      <td>0.195020</td>\n",
       "      <td>0.634557</td>\n",
       "      <td>0.292552</td>\n",
       "      <td>0.372786</td>\n",
       "      <td>0.583364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LS</th>\n",
       "      <td>0.791851</td>\n",
       "      <td>0.763780</td>\n",
       "      <td>0.645191</td>\n",
       "      <td>0.575935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.327094</td>\n",
       "      <td>0.702399</td>\n",
       "      <td>0.413058</td>\n",
       "      <td>0.680435</td>\n",
       "      <td>0.623030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711433</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.550888</td>\n",
       "      <td>-0.139770</td>\n",
       "      <td>0.544188</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.710072</td>\n",
       "      <td>0.366331</td>\n",
       "      <td>0.527504</td>\n",
       "      <td>0.656765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>0.456650</td>\n",
       "      <td>0.456005</td>\n",
       "      <td>0.226665</td>\n",
       "      <td>0.332808</td>\n",
       "      <td>0.327094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452648</td>\n",
       "      <td>0.133390</td>\n",
       "      <td>0.298254</td>\n",
       "      <td>0.386305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335923</td>\n",
       "      <td>0.418549</td>\n",
       "      <td>0.260579</td>\n",
       "      <td>0.168449</td>\n",
       "      <td>0.245768</td>\n",
       "      <td>0.050617</td>\n",
       "      <td>0.492122</td>\n",
       "      <td>0.207772</td>\n",
       "      <td>0.116434</td>\n",
       "      <td>0.365840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNPS</th>\n",
       "      <td>0.919123</td>\n",
       "      <td>0.914988</td>\n",
       "      <td>0.645691</td>\n",
       "      <td>0.665866</td>\n",
       "      <td>0.702399</td>\n",
       "      <td>0.452648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.347775</td>\n",
       "      <td>0.603734</td>\n",
       "      <td>0.739757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661103</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.648086</td>\n",
       "      <td>0.105889</td>\n",
       "      <td>0.493103</td>\n",
       "      <td>0.142737</td>\n",
       "      <td>0.850773</td>\n",
       "      <td>0.379795</td>\n",
       "      <td>0.421773</td>\n",
       "      <td>0.662237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>0.423759</td>\n",
       "      <td>0.390069</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.338214</td>\n",
       "      <td>0.413058</td>\n",
       "      <td>0.133390</td>\n",
       "      <td>0.347775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.447976</td>\n",
       "      <td>0.376628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428512</td>\n",
       "      <td>0.338774</td>\n",
       "      <td>0.302908</td>\n",
       "      <td>-0.074274</td>\n",
       "      <td>0.281727</td>\n",
       "      <td>0.157996</td>\n",
       "      <td>0.360765</td>\n",
       "      <td>0.247335</td>\n",
       "      <td>0.393426</td>\n",
       "      <td>0.391097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <td>0.793812</td>\n",
       "      <td>0.743604</td>\n",
       "      <td>0.545510</td>\n",
       "      <td>0.532190</td>\n",
       "      <td>0.680435</td>\n",
       "      <td>0.298254</td>\n",
       "      <td>0.603734</td>\n",
       "      <td>0.447976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760130</td>\n",
       "      <td>0.709488</td>\n",
       "      <td>0.555117</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.501195</td>\n",
       "      <td>0.191705</td>\n",
       "      <td>0.752697</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.710461</td>\n",
       "      <td>0.865628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>0.840559</td>\n",
       "      <td>0.823196</td>\n",
       "      <td>0.578256</td>\n",
       "      <td>0.601060</td>\n",
       "      <td>0.623030</td>\n",
       "      <td>0.386305</td>\n",
       "      <td>0.739757</td>\n",
       "      <td>0.376628</td>\n",
       "      <td>0.710750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708269</td>\n",
       "      <td>0.766968</td>\n",
       "      <td>0.547674</td>\n",
       "      <td>0.113256</td>\n",
       "      <td>0.519929</td>\n",
       "      <td>0.246220</td>\n",
       "      <td>0.780362</td>\n",
       "      <td>0.457576</td>\n",
       "      <td>0.495908</td>\n",
       "      <td>0.828328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP$</th>\n",
       "      <td>0.915115</td>\n",
       "      <td>0.886477</td>\n",
       "      <td>0.655349</td>\n",
       "      <td>0.631091</td>\n",
       "      <td>0.772538</td>\n",
       "      <td>0.370217</td>\n",
       "      <td>0.772988</td>\n",
       "      <td>0.464109</td>\n",
       "      <td>0.853046</td>\n",
       "      <td>0.789127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798135</td>\n",
       "      <td>0.822329</td>\n",
       "      <td>0.685848</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>0.613984</td>\n",
       "      <td>0.248698</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.522347</td>\n",
       "      <td>0.638492</td>\n",
       "      <td>0.801179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBR</th>\n",
       "      <td>0.412125</td>\n",
       "      <td>0.401003</td>\n",
       "      <td>0.276110</td>\n",
       "      <td>0.331937</td>\n",
       "      <td>0.376811</td>\n",
       "      <td>0.182183</td>\n",
       "      <td>0.384794</td>\n",
       "      <td>0.161050</td>\n",
       "      <td>0.290012</td>\n",
       "      <td>0.329227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315147</td>\n",
       "      <td>0.329477</td>\n",
       "      <td>0.311538</td>\n",
       "      <td>-0.045905</td>\n",
       "      <td>0.320134</td>\n",
       "      <td>0.039492</td>\n",
       "      <td>0.381840</td>\n",
       "      <td>0.138731</td>\n",
       "      <td>0.148097</td>\n",
       "      <td>0.334710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>0.705409</td>\n",
       "      <td>0.691376</td>\n",
       "      <td>0.461609</td>\n",
       "      <td>0.457049</td>\n",
       "      <td>0.501598</td>\n",
       "      <td>0.312397</td>\n",
       "      <td>0.597186</td>\n",
       "      <td>0.302494</td>\n",
       "      <td>0.609055</td>\n",
       "      <td>0.601422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601721</td>\n",
       "      <td>0.678179</td>\n",
       "      <td>0.523422</td>\n",
       "      <td>0.044513</td>\n",
       "      <td>0.382143</td>\n",
       "      <td>0.155876</td>\n",
       "      <td>0.632115</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.418821</td>\n",
       "      <td>0.580207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UH</th>\n",
       "      <td>0.790048</td>\n",
       "      <td>0.737380</td>\n",
       "      <td>0.587884</td>\n",
       "      <td>0.574393</td>\n",
       "      <td>0.711433</td>\n",
       "      <td>0.335923</td>\n",
       "      <td>0.661103</td>\n",
       "      <td>0.428512</td>\n",
       "      <td>0.760130</td>\n",
       "      <td>0.708269</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.701543</td>\n",
       "      <td>0.518537</td>\n",
       "      <td>-0.099647</td>\n",
       "      <td>0.656055</td>\n",
       "      <td>0.291471</td>\n",
       "      <td>0.736120</td>\n",
       "      <td>0.500744</td>\n",
       "      <td>0.625374</td>\n",
       "      <td>0.689402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>0.890128</td>\n",
       "      <td>0.876908</td>\n",
       "      <td>0.615098</td>\n",
       "      <td>0.592037</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.418549</td>\n",
       "      <td>0.816053</td>\n",
       "      <td>0.338774</td>\n",
       "      <td>0.709488</td>\n",
       "      <td>0.766968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.628809</td>\n",
       "      <td>0.063627</td>\n",
       "      <td>0.516544</td>\n",
       "      <td>0.181634</td>\n",
       "      <td>0.827321</td>\n",
       "      <td>0.476414</td>\n",
       "      <td>0.472784</td>\n",
       "      <td>0.718894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBZ</th>\n",
       "      <td>0.704837</td>\n",
       "      <td>0.711894</td>\n",
       "      <td>0.546748</td>\n",
       "      <td>0.497882</td>\n",
       "      <td>0.550888</td>\n",
       "      <td>0.260579</td>\n",
       "      <td>0.648086</td>\n",
       "      <td>0.302908</td>\n",
       "      <td>0.555117</td>\n",
       "      <td>0.547674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518537</td>\n",
       "      <td>0.628809</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073746</td>\n",
       "      <td>0.399886</td>\n",
       "      <td>0.178078</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.369204</td>\n",
       "      <td>0.422658</td>\n",
       "      <td>0.504611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quotes</th>\n",
       "      <td>0.057361</td>\n",
       "      <td>0.064259</td>\n",
       "      <td>-0.007403</td>\n",
       "      <td>-0.008925</td>\n",
       "      <td>-0.139770</td>\n",
       "      <td>0.168449</td>\n",
       "      <td>0.105889</td>\n",
       "      <td>-0.074274</td>\n",
       "      <td>-0.017409</td>\n",
       "      <td>0.113256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099647</td>\n",
       "      <td>0.063627</td>\n",
       "      <td>0.073746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.095087</td>\n",
       "      <td>-0.091167</td>\n",
       "      <td>0.150034</td>\n",
       "      <td>-0.040800</td>\n",
       "      <td>-0.105625</td>\n",
       "      <td>0.087668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questionmarks</th>\n",
       "      <td>0.576761</td>\n",
       "      <td>0.533813</td>\n",
       "      <td>0.483970</td>\n",
       "      <td>0.420193</td>\n",
       "      <td>0.544188</td>\n",
       "      <td>0.245768</td>\n",
       "      <td>0.493103</td>\n",
       "      <td>0.281727</td>\n",
       "      <td>0.501195</td>\n",
       "      <td>0.519929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656055</td>\n",
       "      <td>0.516544</td>\n",
       "      <td>0.399886</td>\n",
       "      <td>-0.095087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.288513</td>\n",
       "      <td>0.510698</td>\n",
       "      <td>0.322962</td>\n",
       "      <td>0.456010</td>\n",
       "      <td>0.471467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclamationmarks</th>\n",
       "      <td>0.203342</td>\n",
       "      <td>0.184856</td>\n",
       "      <td>0.184978</td>\n",
       "      <td>0.195020</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.050617</td>\n",
       "      <td>0.142737</td>\n",
       "      <td>0.157996</td>\n",
       "      <td>0.191705</td>\n",
       "      <td>0.246220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291471</td>\n",
       "      <td>0.181634</td>\n",
       "      <td>0.178078</td>\n",
       "      <td>-0.091167</td>\n",
       "      <td>0.288513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153422</td>\n",
       "      <td>0.183890</td>\n",
       "      <td>0.314763</td>\n",
       "      <td>0.121069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fullstops</th>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.878764</td>\n",
       "      <td>0.614681</td>\n",
       "      <td>0.634557</td>\n",
       "      <td>0.710072</td>\n",
       "      <td>0.492122</td>\n",
       "      <td>0.850773</td>\n",
       "      <td>0.360765</td>\n",
       "      <td>0.752697</td>\n",
       "      <td>0.780362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736120</td>\n",
       "      <td>0.827321</td>\n",
       "      <td>0.556350</td>\n",
       "      <td>0.150034</td>\n",
       "      <td>0.510698</td>\n",
       "      <td>0.153422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479019</td>\n",
       "      <td>0.492784</td>\n",
       "      <td>0.778169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns1st</th>\n",
       "      <td>0.501285</td>\n",
       "      <td>0.457993</td>\n",
       "      <td>0.299160</td>\n",
       "      <td>0.292552</td>\n",
       "      <td>0.366331</td>\n",
       "      <td>0.207772</td>\n",
       "      <td>0.379795</td>\n",
       "      <td>0.247335</td>\n",
       "      <td>0.682861</td>\n",
       "      <td>0.457576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500744</td>\n",
       "      <td>0.476414</td>\n",
       "      <td>0.369204</td>\n",
       "      <td>-0.040800</td>\n",
       "      <td>0.322962</td>\n",
       "      <td>0.183890</td>\n",
       "      <td>0.479019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400419</td>\n",
       "      <td>0.404471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns2nd</th>\n",
       "      <td>0.564170</td>\n",
       "      <td>0.522194</td>\n",
       "      <td>0.426286</td>\n",
       "      <td>0.372786</td>\n",
       "      <td>0.527504</td>\n",
       "      <td>0.116434</td>\n",
       "      <td>0.421773</td>\n",
       "      <td>0.393426</td>\n",
       "      <td>0.710461</td>\n",
       "      <td>0.495908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625374</td>\n",
       "      <td>0.472784</td>\n",
       "      <td>0.422658</td>\n",
       "      <td>-0.105625</td>\n",
       "      <td>0.456010</td>\n",
       "      <td>0.314763</td>\n",
       "      <td>0.492784</td>\n",
       "      <td>0.400419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.515788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns3rd</th>\n",
       "      <td>0.802912</td>\n",
       "      <td>0.774438</td>\n",
       "      <td>0.558371</td>\n",
       "      <td>0.583364</td>\n",
       "      <td>0.656765</td>\n",
       "      <td>0.365840</td>\n",
       "      <td>0.662237</td>\n",
       "      <td>0.391097</td>\n",
       "      <td>0.865628</td>\n",
       "      <td>0.828328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689402</td>\n",
       "      <td>0.718894</td>\n",
       "      <td>0.504611</td>\n",
       "      <td>0.087668</td>\n",
       "      <td>0.471467</td>\n",
       "      <td>0.121069</td>\n",
       "      <td>0.778169</td>\n",
       "      <td>0.404471</td>\n",
       "      <td>0.515788</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  totalWordsCount        FW        JJ       JJR        LS  \\\n",
       "totalWordsCount          1.000000  0.980416  0.696009  0.702140  0.791851   \n",
       "FW                       0.980416  1.000000  0.691682  0.684645  0.763780   \n",
       "JJ                       0.696009  0.691682  1.000000  0.518882  0.645191   \n",
       "JJR                      0.702140  0.684645  0.518882  1.000000  0.575935   \n",
       "LS                       0.791851  0.763780  0.645191  0.575935  1.000000   \n",
       "NN                       0.456650  0.456005  0.226665  0.332808  0.327094   \n",
       "NNPS                     0.919123  0.914988  0.645691  0.665866  0.702399   \n",
       "NNS                      0.423759  0.390069  0.296100  0.338214  0.413058   \n",
       "POS                      0.793812  0.743604  0.545510  0.532190  0.680435   \n",
       "PRP                      0.840559  0.823196  0.578256  0.601060  0.623030   \n",
       "PRP$                     0.915115  0.886477  0.655349  0.631091  0.772538   \n",
       "RBR                      0.412125  0.401003  0.276110  0.331937  0.376811   \n",
       "RBS                      0.705409  0.691376  0.461609  0.457049  0.501598   \n",
       "UH                       0.790048  0.737380  0.587884  0.574393  0.711433   \n",
       "VBD                      0.890128  0.876908  0.615098  0.592037  0.644860   \n",
       "VBZ                      0.704837  0.711894  0.546748  0.497882  0.550888   \n",
       "Quotes                   0.057361  0.064259 -0.007403 -0.008925 -0.139770   \n",
       "questionmarks            0.576761  0.533813  0.483970  0.420193  0.544188   \n",
       "exclamationmarks         0.203342  0.184856  0.184978  0.195020  0.179800   \n",
       "fullstops                0.919200  0.878764  0.614681  0.634557  0.710072   \n",
       "pronouns1st              0.501285  0.457993  0.299160  0.292552  0.366331   \n",
       "pronouns2nd              0.564170  0.522194  0.426286  0.372786  0.527504   \n",
       "pronouns3rd              0.802912  0.774438  0.558371  0.583364  0.656765   \n",
       "\n",
       "                        NN      NNPS       NNS       POS       PRP  ...  \\\n",
       "totalWordsCount   0.456650  0.919123  0.423759  0.793812  0.840559  ...   \n",
       "FW                0.456005  0.914988  0.390069  0.743604  0.823196  ...   \n",
       "JJ                0.226665  0.645691  0.296100  0.545510  0.578256  ...   \n",
       "JJR               0.332808  0.665866  0.338214  0.532190  0.601060  ...   \n",
       "LS                0.327094  0.702399  0.413058  0.680435  0.623030  ...   \n",
       "NN                1.000000  0.452648  0.133390  0.298254  0.386305  ...   \n",
       "NNPS              0.452648  1.000000  0.347775  0.603734  0.739757  ...   \n",
       "NNS               0.133390  0.347775  1.000000  0.447976  0.376628  ...   \n",
       "POS               0.298254  0.603734  0.447976  1.000000  0.710750  ...   \n",
       "PRP               0.386305  0.739757  0.376628  0.710750  1.000000  ...   \n",
       "PRP$              0.370217  0.772988  0.464109  0.853046  0.789127  ...   \n",
       "RBR               0.182183  0.384794  0.161050  0.290012  0.329227  ...   \n",
       "RBS               0.312397  0.597186  0.302494  0.609055  0.601422  ...   \n",
       "UH                0.335923  0.661103  0.428512  0.760130  0.708269  ...   \n",
       "VBD               0.418549  0.816053  0.338774  0.709488  0.766968  ...   \n",
       "VBZ               0.260579  0.648086  0.302908  0.555117  0.547674  ...   \n",
       "Quotes            0.168449  0.105889 -0.074274 -0.017409  0.113256  ...   \n",
       "questionmarks     0.245768  0.493103  0.281727  0.501195  0.519929  ...   \n",
       "exclamationmarks  0.050617  0.142737  0.157996  0.191705  0.246220  ...   \n",
       "fullstops         0.492122  0.850773  0.360765  0.752697  0.780362  ...   \n",
       "pronouns1st       0.207772  0.379795  0.247335  0.682861  0.457576  ...   \n",
       "pronouns2nd       0.116434  0.421773  0.393426  0.710461  0.495908  ...   \n",
       "pronouns3rd       0.365840  0.662237  0.391097  0.865628  0.828328  ...   \n",
       "\n",
       "                        UH       VBD       VBZ    Quotes  questionmarks  \\\n",
       "totalWordsCount   0.790048  0.890128  0.704837  0.057361       0.576761   \n",
       "FW                0.737380  0.876908  0.711894  0.064259       0.533813   \n",
       "JJ                0.587884  0.615098  0.546748 -0.007403       0.483970   \n",
       "JJR               0.574393  0.592037  0.497882 -0.008925       0.420193   \n",
       "LS                0.711433  0.644860  0.550888 -0.139770       0.544188   \n",
       "NN                0.335923  0.418549  0.260579  0.168449       0.245768   \n",
       "NNPS              0.661103  0.816053  0.648086  0.105889       0.493103   \n",
       "NNS               0.428512  0.338774  0.302908 -0.074274       0.281727   \n",
       "POS               0.760130  0.709488  0.555117 -0.017409       0.501195   \n",
       "PRP               0.708269  0.766968  0.547674  0.113256       0.519929   \n",
       "PRP$              0.798135  0.822329  0.685848 -0.006570       0.613984   \n",
       "RBR               0.315147  0.329477  0.311538 -0.045905       0.320134   \n",
       "RBS               0.601721  0.678179  0.523422  0.044513       0.382143   \n",
       "UH                1.000000  0.701543  0.518537 -0.099647       0.656055   \n",
       "VBD               0.701543  1.000000  0.628809  0.063627       0.516544   \n",
       "VBZ               0.518537  0.628809  1.000000  0.073746       0.399886   \n",
       "Quotes           -0.099647  0.063627  0.073746  1.000000      -0.095087   \n",
       "questionmarks     0.656055  0.516544  0.399886 -0.095087       1.000000   \n",
       "exclamationmarks  0.291471  0.181634  0.178078 -0.091167       0.288513   \n",
       "fullstops         0.736120  0.827321  0.556350  0.150034       0.510698   \n",
       "pronouns1st       0.500744  0.476414  0.369204 -0.040800       0.322962   \n",
       "pronouns2nd       0.625374  0.472784  0.422658 -0.105625       0.456010   \n",
       "pronouns3rd       0.689402  0.718894  0.504611  0.087668       0.471467   \n",
       "\n",
       "                  exclamationmarks  fullstops  pronouns1st  pronouns2nd  \\\n",
       "totalWordsCount           0.203342   0.919200     0.501285     0.564170   \n",
       "FW                        0.184856   0.878764     0.457993     0.522194   \n",
       "JJ                        0.184978   0.614681     0.299160     0.426286   \n",
       "JJR                       0.195020   0.634557     0.292552     0.372786   \n",
       "LS                        0.179800   0.710072     0.366331     0.527504   \n",
       "NN                        0.050617   0.492122     0.207772     0.116434   \n",
       "NNPS                      0.142737   0.850773     0.379795     0.421773   \n",
       "NNS                       0.157996   0.360765     0.247335     0.393426   \n",
       "POS                       0.191705   0.752697     0.682861     0.710461   \n",
       "PRP                       0.246220   0.780362     0.457576     0.495908   \n",
       "PRP$                      0.248698   0.817857     0.522347     0.638492   \n",
       "RBR                       0.039492   0.381840     0.138731     0.148097   \n",
       "RBS                       0.155876   0.632115     0.392571     0.418821   \n",
       "UH                        0.291471   0.736120     0.500744     0.625374   \n",
       "VBD                       0.181634   0.827321     0.476414     0.472784   \n",
       "VBZ                       0.178078   0.556350     0.369204     0.422658   \n",
       "Quotes                   -0.091167   0.150034    -0.040800    -0.105625   \n",
       "questionmarks             0.288513   0.510698     0.322962     0.456010   \n",
       "exclamationmarks          1.000000   0.153422     0.183890     0.314763   \n",
       "fullstops                 0.153422   1.000000     0.479019     0.492784   \n",
       "pronouns1st               0.183890   0.479019     1.000000     0.400419   \n",
       "pronouns2nd               0.314763   0.492784     0.400419     1.000000   \n",
       "pronouns3rd               0.121069   0.778169     0.404471     0.515788   \n",
       "\n",
       "                  pronouns3rd  \n",
       "totalWordsCount      0.802912  \n",
       "FW                   0.774438  \n",
       "JJ                   0.558371  \n",
       "JJR                  0.583364  \n",
       "LS                   0.656765  \n",
       "NN                   0.365840  \n",
       "NNPS                 0.662237  \n",
       "NNS                  0.391097  \n",
       "POS                  0.865628  \n",
       "PRP                  0.828328  \n",
       "PRP$                 0.801179  \n",
       "RBR                  0.334710  \n",
       "RBS                  0.580207  \n",
       "UH                   0.689402  \n",
       "VBD                  0.718894  \n",
       "VBZ                  0.504611  \n",
       "Quotes               0.087668  \n",
       "questionmarks        0.471467  \n",
       "exclamationmarks     0.121069  \n",
       "fullstops            0.778169  \n",
       "pronouns1st          0.404471  \n",
       "pronouns2nd          0.515788  \n",
       "pronouns3rd          1.000000  \n",
       "\n",
       "[23 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer of 100-150 words in this cell.\n",
    "\n",
    "The first assumption is not always true. This assumption means that with continuous data features, Naïve Bayes makes every class’s value distributed, because of the normal distribution. But in our data, for the continuous data, we segment the data by the class and calculate the mean and standard deviation of the class. In the upper histogram, the result is obviously not normal distribution.\n",
    "The independence assumption means no pair of features are dependent. This assumption is not always true. For example, we have “weight” and “son” which is not associated, but “height” , “weight” and “body mass index” are connected with each other. They are not conditional independent class. We can see from data.corr(), for example, the correlation of “FW” and “JJ” is 0.696009. They are obvious not independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Feature Selection and Ethics [5 marks]\n",
    "- Question 4.1: [3.5 marks]\n",
    "- Question 4.2: [1.5 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital-status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Female</td>\n",
       "      <td>Philippines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2303 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label    Workclass     Education      Marital-status        Occupation  \\\n",
       "0     <=50K  Federal-gov     Bachelors       Never-married   Exec-managerial   \n",
       "1     <=50K      Private     Bachelors       Never-married   Exec-managerial   \n",
       "2      >50K      Private       HS-grad  Married-civ-spouse  Transport-moving   \n",
       "3     <=50K      Private       HS-grad       Never-married   Farming-fishing   \n",
       "4     <=50K      Private     Bachelors  Married-civ-spouse    Prof-specialty   \n",
       "...     ...          ...           ...                 ...               ...   \n",
       "2298   >50K      Private       Masters            Divorced    Prof-specialty   \n",
       "2299  <=50K      Private     Bachelors  Married-civ-spouse    Prof-specialty   \n",
       "2300  <=50K      Private     Bachelors           Separated   Exec-managerial   \n",
       "2301  <=50K      Private  Some-college       Never-married    Prof-specialty   \n",
       "2302  <=50K      Private       HS-grad  Married-civ-spouse      Craft-repair   \n",
       "\n",
       "        Relationship                Race     Sex Native-country  \n",
       "0      Not-in-family               Black  Female  United-States  \n",
       "1          Own-child               White  Female  United-States  \n",
       "2            Husband               Black    Male  United-States  \n",
       "3          Own-child               White    Male  United-States  \n",
       "4     Other-relative  Asian-Pac-Islander  Female    Philippines  \n",
       "...              ...                 ...     ...            ...  \n",
       "2298       Own-child               White  Female  United-States  \n",
       "2299         Husband               White    Male  United-States  \n",
       "2300   Not-in-family               White  Female  United-States  \n",
       "2301   Not-in-family               White    Male  United-States  \n",
       "2302         Husband               White    Male  United-States  \n",
       "\n",
       "[2303 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4 =  preprocess(\"adult_nominal_for_q4.csv\")\n",
    "ls = list(data4[data4.values == \"?\"].index)\n",
    "data4 = data4.drop(index = ls, axis = 0)\n",
    "data4 = data4.reset_index(drop = True)\n",
    "data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital-status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2303 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Workclass  Education  Marital-status  Occupation  Relationship  Race  \\\n",
       "0             0          9               3           3             1     2   \n",
       "1             2          9               3           3             3     4   \n",
       "2             2         11               1          13             0     2   \n",
       "3             2         11               3           4             3     4   \n",
       "4             2          9               1           9             2     1   \n",
       "...         ...        ...             ...         ...           ...   ...   \n",
       "2298          2         12               0           9             3     4   \n",
       "2299          2          9               1           9             0     4   \n",
       "2300          2          9               4           3             1     4   \n",
       "2301          2         15               3           9             1     4   \n",
       "2302          2         11               1           2             0     4   \n",
       "\n",
       "      Sex  Native-country  \n",
       "0       0              33  \n",
       "1       0              33  \n",
       "2       1              33  \n",
       "3       1              33  \n",
       "4       0              25  \n",
       "...   ...             ...  \n",
       "2298    0              33  \n",
       "2299    1              33  \n",
       "2300    0              33  \n",
       "2301    1              33  \n",
       "2302    1              33  \n",
       "\n",
       "[2303 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "data4_x = data4.drop(columns = \"Label\").apply(le.fit_transform)\n",
    "data4_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00949767, 0.06958636, 0.10066801, 0.06259536, 0.11003439,\n",
       "       0.00456488, 0.02221777, 0.012301  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "score = mutual_info_classif(data4_x, data4.Label, discrete_features=True)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer to **Question 4.1** of 100-150 words in this cell.\n",
    "\n",
    "Firstly, I got a list to store all lines have \"?\", and then delete these lines and resort it. Because all of the information needed to be transfer to numeric, I use preprocessing from sklearn to transfer all feature to numeric. In this case, we can use mutual_info_classif to calculate the maximum feature. This maximum feature is the most valuable, “Relationship”, is the one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your text answer to **Question 4.2** of 50-100 words in this cell.\n",
    "\n",
    "To some perspective, this method is not very suitable in reality. Because some item is really sensitive. For example, “sex”, “native country” and “race”. If we got the Female have less salary, this may cause some objective bias. So, it is not suitable to choose these features. It is better to choose not sensitive features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: [Han Wang 1041260]\n",
    "   \n",
    "   <b>Dated</b>: [2022/4/5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
